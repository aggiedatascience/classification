{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Checkpoint #1: *Open the Notebook and Load the Data***\n"
      ],
      "metadata": {
        "id": "Qdi7b0NNQtGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to https://docs.google.com/presentation/d/1z9HhCQ2ldON_tb9JLuJlb_bH9cghNhHwXCE-nqxbaUI/edit?usp=sharing for help on running the notebook in Colab."
      ],
      "metadata": {
        "id": "su2XI6qxZuRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gtN-dQE9MilS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/Regression\\ Workshop"
      ],
      "metadata": {
        "id": "md-tXurDMzsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "GZPFKz5hYj0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRtodPr-MZ7N"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('2016_Citizen_Survey_Master_Dataset_20240218.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_shape = ...\n",
        "\n",
        "print(f\"Initial shape of the dataframe: {df_shape}\")"
      ],
      "metadata": {
        "id": "ZEneBYs8BmOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checkpoint #2: *Prepare the Data with Imputation and Encoding***\n"
      ],
      "metadata": {
        "id": "N_iGJNc2RbRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation\n",
        "\n",
        "Data imputation refers to the replacement of null values with some other value, whether it be the mean of a quantitative variable or the mode of a qualitative variable.\n",
        "\n",
        "Here are some helpful links:\n",
        "\n",
        "* https://www.tutorialspoint.com/how-to-handle-missing-values-of-categorical-variables-in-python#:~:text=in%20the%20section.-,Fill%20the%20missing%20values%20with%20the%20computed%20mode%20using%20the,method_name%20parameter%20as%20'mode'\n",
        "* https://saturncloud.io/blog/how-to-count-nan-and-null-values-in-a-pandas-dataframe/"
      ],
      "metadata": {
        "id": "s0aV8mVkZEyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the total number of nulls in the entire dataframe\n",
        "nulls = ...\n",
        "\n",
        "print(f\"Total number of nulls in dataset (original): {nulls}\")"
      ],
      "metadata": {
        "id": "Vj2wqXvkYqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Evaluation:\n",
        "# Shape\n",
        "# remove null values\n",
        "# convert categorical values, etc.\n",
        "\n",
        "# Checkpoint 1: clean / drop null values\n",
        "# rename values and columns (mainly area column )\n",
        "# Handling null values\n",
        "\n",
        "# DON'T REMOVE\n",
        "df.drop(columns=[\"Information preference about city government activities - Other\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "for column in df.columns:\n",
        "  num_nulls_in_column = ...                     # get the number of nulls in the column (modify the line from last cell)\n",
        "  if num_nulls_in_column != 0:\n",
        "    column_dtype = ...                          # get the dtype of this column (look this up)\n",
        "    if column_dtype == 'object':                # categorical columns\n",
        "      mode = ...                                # calculate mode of column\n",
        "      df[column] = ...                          # fill all null values in the column with this mode\n",
        "    elif column_dtype in ('float64', 'int64'):  # numerical columns\n",
        "      mean = ...                                # calculate mode of column\n",
        "      df[column] = ...                          # same as line above (fill nulls)\n",
        "\n",
        "new_nulls = ...\n",
        "print(f\"Total number of nulls in dataset (after imputation): {new_nulls}\")"
      ],
      "metadata": {
        "id": "oIKR5eCIMg9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "\n",
        "Encoding aims to handle categorical data and convert it into quantitative data that the model can understand."
      ],
      "metadata": {
        "id": "mFrCY0qIIgSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the 'Area of College Station' to 'area' and do it inplace\n",
        "...\n",
        "\n",
        "y = df[\"area\"]"
      ],
      "metadata": {
        "id": "8LWImWY7lFZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_categorical_before = df.select_dtypes(include=['object']).shape[1]\n",
        "print(f\"Number of categorical variables before encoding: {num_categorical_before}\")"
      ],
      "metadata": {
        "id": "f_zPUK4ZY4P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine the encoding technique for a column based on the number of unique values\n",
        "# Dont worry abt this but if unique values > 10, then were gonna use a frequency encoding stategy good for high-cardinality categorical variables\n",
        "def determine_encoding(column):\n",
        "    unique_values = column.nunique()\n",
        "    # If the number of unique values is too high, return None to avoid encoding\n",
        "    if unique_values > 10:\n",
        "        return None\n",
        "    elif unique_values == 2:  # for binary columns\n",
        "        return \"Label\"\n",
        "    else:\n",
        "        return \"OneHot\"\n",
        "\n",
        "X = df.drop(columns=[\"area\"], inplace=False, errors=\"ignore\")\n",
        "\n",
        "one_hot_columns = []\n",
        "drop_columns = []\n",
        "\n",
        "for col in X.columns:\n",
        "  if X[col].dtype == 'object':            # Check if the column is categorical\n",
        "    encoding_type = ...                   # use function to determine which encoding\n",
        "    if encoding_type == \"Label\":\n",
        "      le = ...                            # instantiate a label encoder object (default params)\n",
        "      X[col] = ...                        # fit/transform the column in X and reassign\n",
        "    elif encoding_type == \"OneHot\":\n",
        "      one_hot_columns.append(col)\n",
        "    else:\n",
        "      drop_columns.append(col)\n",
        "\n",
        "# perform One-Hot encoding with dummies using one_hot_columns list (look up \"pandas dummies\")\n",
        "X = ...\n",
        "\n",
        "encoded_shape = X.shape\n",
        "print(f\"Shape of the Encoded DataFrame: {encoded_shape}\")\n",
        "\n",
        "num_categorical_after = X.select_dtypes(include=['object']).shape[1]\n",
        "print(f\"Number of categorical variables after encoding: {num_categorical_after}\")"
      ],
      "metadata": {
        "id": "wrcQVe_LYmkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there are still some categorical variables left, specifically high cardinality variables\n",
        "\n",
        "# select columns with dtype of 'object' --> look up \"pandas select dtypes\"\n",
        "remaining_categorical_vars = ...\n",
        "\n",
        "# write lambda expression to see if number of unique values is greater than 10 for each column in remaining_categorical_vars\n",
        "columns_with_many_categories = remaining_categorical_vars.apply(...)\n",
        "\n",
        "# trying out frequency encoding:\n",
        "columns_to_encode = columns_with_many_categories[columns_with_many_categories].index.tolist()\n",
        "\n",
        "for col in columns_to_encode:\n",
        "    frequency = ...                   # frequency of each category with normalization --> look up \"value_counts\"\n",
        "    X[col] = X[col].map(frequency)\n",
        "\n",
        "num_categorical_after = X.select_dtypes(include=['object']).shape[1]\n",
        "print(f\"Number of categorical variables after encoding: {num_categorical_after}\") #now there's 0 categorical variables"
      ],
      "metadata": {
        "id": "H_YsMWvfE1VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checkpoint #3: *Run the Base Models***\n"
      ],
      "metadata": {
        "id": "GPXj3PkoRpuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "aHNH2CtmFM7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset\n",
        "# test size should be 0.2 and random state should be 42\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "# Checking the shapes of the resulting splits\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "sUUFNj696qca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_predict_eval(model):\n",
        "  # fit model on training data\n",
        "  ...\n",
        "\n",
        "  # predict y's for x-test\n",
        "  model_pred = ...\n",
        "\n",
        "  # create classification report from predictions and actual y's --> look up on Google\n",
        "  model_report = ...\n",
        "\n",
        "  # create confusion matrix from predictions and actual y's --> look up on Google\n",
        "  model_matrix = ...\n",
        "\n",
        "  return model_report, model_matrix"
      ],
      "metadata": {
        "id": "vF6M_80bR7vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We show the example with Logistic Regression and Random Forest, which have very high performance."
      ],
      "metadata": {
        "id": "oAwiUWI3HOIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()   #Penalty/regularization is optional\n",
        "\n",
        "lr_report, lr_matrix = fit_predict_eval(lr)\n",
        "\n",
        "print(lr_report)\n",
        "print(lr_matrix)"
      ],
      "metadata": {
        "id": "lsBF_RoXSJqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier()   #Penalty/regularization is optional\n",
        "\n",
        "rfc_report, rfc_matrix = fit_predict_eval(rfc)\n",
        "\n",
        "print(rfc_report)\n",
        "print(rfc_matrix)"
      ],
      "metadata": {
        "id": "2a0nDpQsBjkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, try creating the models with K-Nearest Neighbors and Support Vector Classifiers**. Look up on google for help on creating model object"
      ],
      "metadata": {
        "id": "x1ZOUy5uHdmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = ...\n",
        "\n",
        "knn_report, knn_matrix = fit_predict_eval(knn)\n",
        "\n",
        "print(knn_report)\n",
        "print(knn_matrix)"
      ],
      "metadata": {
        "id": "X1bjL9mq47H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = ...\n",
        "\n",
        "svm_report, svm_matrix = fit_predict_eval(svm)\n",
        "\n",
        "print(svm_report)\n",
        "print(svm_matrix)"
      ],
      "metadata": {
        "id": "oFiSMg7pL3dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checkpoint #4: *Fine-Tune the Hyperparameters***\n"
      ],
      "metadata": {
        "id": "IxOQyC36UMTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finally, it's your turn to create the best model using SVM or KNN! Alternatively, try to improve the LogisticRegression parameters to get an even better performance!** Try different models with different hyperparameters."
      ],
      "metadata": {
        "id": "DuR_9kr1Hyvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "eP6xFw_jt11h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}